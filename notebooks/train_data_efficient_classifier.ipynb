{
  "cells": [
    {
      "metadata": {
        "id": "JIpyR7i-XtMy"
      },
      "cell_type": "markdown",
      "source": [
        "~~~\n",
        "Copyright 2024 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/cxr-foundation/blob/master/notebooks/train_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fcxr-foundation%2Fmaster%2Fnotebooks%2Ftrain_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/cxr-foundation/blob/master/notebooks/train_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/google/cxr-foundation\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n"
      ]
    },
    {
      "metadata": {
        "id": "OTTGbllChDdY"
      },
      "cell_type": "markdown",
      "source": [
        "# CXR Foundation Demo\n",
        "\n",
        "This notebook demonstrates the richness of information contained in embeddings, generated from full Chest X-Ray images. The contents include how to:\n",
        "\n",
        "- Download precomputed embeddings and labels based on the open-access NIH ChestX-ray14 dataset\n",
        "- Train a simple neural network (WITHOUT needing GPU) to detect a medical finding in the embeddings\n",
        "\n",
        "The embeddings are the *elixr_img_contrastive* which are text-aligned image embedding from the Q-former output in ELIXR (https://arxiv.org/abs/2308.01317), can be used for image retrieval.\n",
        "\n",
        "**NOTE:**  To streamline this Colab demonstration and eliminate the need for lengthy downloads, we've precomputed the embeddings, which are considerably smaller in size (similar to compressed images). You can learn how to generate embeddings using the other [notebooks](https://github.com/Google-Health/cxr-foundation/tree/master/notebooks)."
      ]
    },
    {
      "metadata": {
        "id": "Z6Cbq7e2cEzU"
      },
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "\n",
        "Install the CXR Foundation package"
      ]
    },
    {
      "metadata": {
        "id": "sAZeXrk9-u8R",
        "outputId": "5a1751e6-0677-4549-f91b-108f2458f9a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Notebook specific dependencies. Show details only if installing.\n",
        "!pip install numpy==1.26.4 matplotlib tf-models-official==2.14.0 > /dev/null 2>&1 && echo \"Installation successful\" || (>&2 pip install matplotlib tf-models-official==2.14.0 > /dev/null)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-text~=2.14.0 (from tf-models-official) (from versions: 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-text~=2.14.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "_sNEw2facQnr"
      },
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: If you are using Colab, you must restart the runtime after installing new packages.\n",
        "\n",
        "NOTE: There will be some ERROR messages due to the protobuf library - this is normal."
      ]
    },
    {
      "metadata": {
        "id": "xll2FURLgi4l"
      },
      "cell_type": "markdown",
      "source": [
        "# Download Data\n",
        "\n",
        "The NIH ChestX-ray14 dataset, consists of over 100,000 de-identified images of chest x-rays, with fourteen common disease labels, text-mined from the text radiological reports via NLP techniques. The dataset is available on the NIH [download site](https://nihcc.app.box.com/v/ChestXray-NIHCC) and on [Google Cloud](https://cloud.google.com/healthcare-api/docs/resources/public-datasets/nih-chest)."
      ]
    },
    {
      "metadata": {
        "id": "Evn9WNBSd-CP"
      },
      "cell_type": "code",
      "source": [
        "# @title Download labels and precomputed embeddings\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "!wget -nc https://storage.googleapis.com/healthai-us/encoded-data/nih/radiology/cxr/precomputed_image_embeddings.npz https://storage.googleapis.com/healthai-us/encoded-data/nih/radiology/cxr/cxr14_subset_labels.csv https://storage.googleapis.com/healthai-us/encoded-data/nih/radiology/cxr/thumbnails_id_to_webp.npz\n",
        "\n",
        "# Download labels file.\n",
        "# There is a column for each of several findings, which indicate whether or not\n",
        "# the condition is present in the image file.\n",
        "full_labels_df = pd.read_csv(\"cxr14_subset_labels.csv\")\n",
        "display(full_labels_df.head())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "SiFTgj8Icyyl"
      },
      "cell_type": "code",
      "source": [
        "# @title Select below the diagnosis and number of cases to use per label:\n",
        "\n",
        "DIAGNOSIS = \"FRACTURE\" # @param [\"AIRSPACE_OPACITY\", \"FRACTURE\", \"PNEUMOTHORAX\", \"CONSOLIDATION\", \"EFFUSION\", \"PULMONARY_EDEMA\", \"ATELECTASIS\", \"CARDIOMEGALY\"]\n",
        "MAX_CASES_PER_CATEGORY = 400 # @param {type:\"slider\", min:50, max:400, step:5}\n",
        "\n",
        "# Labels df of relevant files\n",
        "df_labels = pd.concat((full_labels_df[full_labels_df[DIAGNOSIS]==0][:MAX_CASES_PER_CATEGORY],\n",
        "                      full_labels_df[full_labels_df[DIAGNOSIS]==1][:MAX_CASES_PER_CATEGORY]), ignore_index=True)\n",
        "\n",
        "print(f\"Selected {MAX_CASES_PER_CATEGORY} positive and {MAX_CASES_PER_CATEGORY} negative cases\")\n",
        "df_labels[[\"image_id\", DIAGNOSIS, \"dicom_file\"]]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-e9SeKUzTnSL"
      },
      "cell_type": "markdown",
      "source": [
        "# Util functions and imports"
      ]
    },
    {
      "metadata": {
        "id": "Rqdct5F0VFPw"
      },
      "cell_type": "markdown",
      "source": [
        "# Fetch Embeddings\n",
        "\n",
        "## Storage Format\n",
        "\n",
        "The embeddings are stored as a .npz file, with the keys representing the image_id (e.g., 00018329_008). The NPZ file format is used to store NumPy arrays in a compressed form, providing an efficient way to manage large datasets containing multiple arrays."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "bL0rpPkb4Gg0"
      },
      "cell_type": "code",
      "source": [
        "# @title Parse Precomputed Embeddings\n",
        "import numpy as np\n",
        "\n",
        "embeddings_file = np.load(\"precomputed_image_embeddings.npz\")\n",
        "df_labels['embeddings'] = df_labels['image_id'].apply(lambda x: embeddings_file[x])\n",
        "embeddings_file.close()\n",
        "\n",
        "display(df_labels[['image_id', 'embeddings']])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "QoHZSZO_hDdl"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare Data for Model Training"
      ]
    },
    {
      "metadata": {
        "id": "Wr8tsaTLNpEH"
      },
      "cell_type": "code",
      "source": [
        "# @title Separate into training, validation, and testing sets.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TEST_SPLIT = 0.1 # @param {type:\"slider\", min:0.05, max:0.8, step:0.05}\n",
        "\n",
        "df_train, df_validate = train_test_split(df_labels, test_size=TEST_SPLIT)\n",
        "\n",
        "print(f\"Training set size: {len(df_train)}\")\n",
        "print(f\"Validation set size: {len(df_validate)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fVGYhxEkWBhs"
      },
      "cell_type": "markdown",
      "source": [
        "## Train a Model\n",
        "\n",
        "Finally, we can train a model using the embeddings! With a simple feed-forward neural network, it should take < 5 minutes to train 100 epochs! No GPU required."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "i6KeAAEJcyym"
      },
      "cell_type": "code",
      "source": [
        "# @title Create Tensorflow Dataset & Model functions\n",
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "import numpy as np\n",
        "from typing import Dict, Iterable\n",
        "from official.modeling.optimization import lars\n",
        "\n",
        "\n",
        "def create_tf_dataset_from_embeddings(\n",
        "    embeddings: Iterable[np.ndarray],\n",
        "    labels: Iterable[int],\n",
        "    embeddings_size: int\n",
        ") -> tf.data.Dataset:\n",
        "    \"\"\"Create a tf.data.Dataset from embeddings, image IDs, and labels.\"\"\"\n",
        "    # Ensure embeddings, image_ids, and labels are lists\n",
        "    embeddings = list(embeddings)\n",
        "    labels = list(labels)\n",
        "\n",
        "    # Check that the lengths match\n",
        "    assert len(embeddings) == len(labels), \\\n",
        "        \"Lengths of embeddings, and labels must be equal\"\n",
        "\n",
        "    # Convert embeddings to np.float32 if necessary\n",
        "    embeddings = [np.asarray(e, dtype=np.float32) for e in embeddings]\n",
        "\n",
        "    # Create datasets for embeddings and labels\n",
        "    ds_embeddings = tf.data.Dataset.from_tensor_slices(embeddings)\n",
        "    ds_labels = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "    # Zip embeddings and labels into a single dataset\n",
        "    dataset = tf.data.Dataset.zip((ds_embeddings, ds_labels))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def create_model(heads,\n",
        "                 token_num,\n",
        "                 embeddings_size,\n",
        "                 learning_rate=0.1,\n",
        "                 end_lr_factor=1.0,\n",
        "                 dropout=0.0,\n",
        "                 decay_steps=1000,\n",
        "                 loss_weights=None,\n",
        "                 hidden_layer_sizes=[512, 256],\n",
        "                 weight_decay=0.0,\n",
        "                 seed=None) -> tf.keras.Model:\n",
        "  \"\"\"\n",
        "  Creates linear probe or multilayer perceptron using LARS + cosine decay.\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(token_num * embeddings_size,))\n",
        "  inputs_reshape = tf.keras.layers.Reshape((token_num, embeddings_size))(inputs)\n",
        "  inputs_pooled = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(inputs_reshape)\n",
        "  hidden = inputs_pooled\n",
        "  # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "  for size in hidden_layer_sizes:\n",
        "    hidden = tf.keras.layers.Dense(\n",
        "        size,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay))(\n",
        "            hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "  output = tf.keras.layers.Dense(\n",
        "      units=len(heads),\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.HeUniform(seed=seed))(\n",
        "          hidden)\n",
        "\n",
        "  outputs = {}\n",
        "  for i, head in enumerate(heads):\n",
        "    outputs[head] = tf.keras.layers.Lambda(\n",
        "        lambda x: x[..., i:i + 1], name=head.lower())(\n",
        "            output)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  learning_rate_fn = tf.keras.experimental.CosineDecay(\n",
        "      tf.cast(learning_rate, tf.float32),\n",
        "      tf.cast(decay_steps, tf.float32),\n",
        "      alpha=tf.cast(end_lr_factor, tf.float32))\n",
        "  model.compile(\n",
        "      optimizer=tfm.optimization.lars.LARS(\n",
        "          learning_rate=learning_rate_fn),\n",
        "      loss=dict([(head, 'binary_crossentropy') for head in heads]),\n",
        "      loss_weights=loss_weights or dict([(head, 1.) for head in heads]),\n",
        "      weighted_metrics=[\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.AUC(curve='PR', name='auc_pr')])\n",
        "  return model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "u0UbFRBncyym"
      },
      "cell_type": "code",
      "source": [
        "# @title Prepare datasets & run training a TensorFlow model for diagnosis prediction\n",
        "\n",
        "# Constants defining the number of tokens and the size of each embedding vector\n",
        "TOKEN_NUM = 32\n",
        "EMBEDDINGS_SIZE = 128\n",
        "\n",
        "# Prepare the training and validation datasets using embeddings and diagnosis labels\n",
        "training_data = create_tf_dataset_from_embeddings(\n",
        "    embeddings=df_train[\"embeddings\"].values,\n",
        "    labels=df_train[DIAGNOSIS].values,\n",
        "    embeddings_size=TOKEN_NUM * EMBEDDINGS_SIZE)\n",
        "\n",
        "validation_data = create_tf_dataset_from_embeddings(\n",
        "    embeddings=df_validate[\"embeddings\"].values,\n",
        "    labels=df_validate[DIAGNOSIS].values,\n",
        "    embeddings_size=TOKEN_NUM * EMBEDDINGS_SIZE)\n",
        "\n",
        "# Create the model with the specified configuration\n",
        "model = create_model(\n",
        "    [DIAGNOSIS],\n",
        "    token_num=TOKEN_NUM,\n",
        "    embeddings_size = EMBEDDINGS_SIZE,\n",
        ")\n",
        "\n",
        "# Train the model using the prepared datasets, with specific batch sizes and caching strategies\n",
        "model.fit(\n",
        "    x=training_data.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=validation_data.batch(1).cache(),\n",
        "    epochs=100,\n",
        ")\n",
        "\n",
        "# Display the model architecture summary\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oNpdUNuqyWk9"
      },
      "cell_type": "markdown",
      "source": [
        "## Examine metrics"
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "pxr52PYuybZp"
      },
      "cell_type": "code",
      "source": [
        "# @title Organize the output and display a sample of the predictions\n",
        "\n",
        "rows = []\n",
        "\n",
        "for embeddings, label in validation_data.batch(1):\n",
        "  row = {\n",
        "      f'{DIAGNOSIS}_prediction': model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "      f'{DIAGNOSIS}_value': label.numpy().flatten()[0]\n",
        "  }\n",
        "  rows.append(row)\n",
        "\n",
        "eval_df = pd.DataFrame(rows)\n",
        "eval_df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Eh8MKqF8mdjp"
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)\n",
        "\n",
        "%matplotlib inline\n",
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "J4wOUaNNIsIg"
      },
      "cell_type": "code",
      "source": [
        "# @title Show Sample Images (prediction scores vs label)\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "import io\n",
        "import pandas as pd\n",
        "from google.colab import output\n",
        "\n",
        "NUM_BUCKETS=5\n",
        "\n",
        "thumbnails = np.load(\"thumbnails_id_to_webp.npz\", allow_pickle=True)\n",
        "\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "sorted_indices = np.argsort(predictions)\n",
        "sorted_predictions = predictions[sorted_indices]\n",
        "sorted_labels = labels[sorted_indices]\n",
        "sorted_image_id = df_validate[\"image_id\"].values[sorted_indices]\n",
        "sorted_image_id, sorted_labels, sorted_predictions\n",
        "\n",
        "\n",
        "def define_buckets(predictions, num_buckets):\n",
        "    quantiles = pd.Series(predictions).quantile([i / num_buckets for i in range(num_buckets + 1)]).tolist()\n",
        "    # Make sure the range covers the last value inclusively\n",
        "    quantiles[-1] = quantiles[-1] + 0.01\n",
        "    return [(quantiles[i], quantiles[i + 1]) for i in range(len(quantiles) - 1)]\n",
        "\n",
        "def bucket_images(sorted_predictions, sorted_labels, num_buckets=4):\n",
        "    # Define buckets\n",
        "    buckets = define_buckets(sorted_predictions, num_buckets)\n",
        "    bucketed_images = {bucket: {0: [], 1: []} for bucket in buckets}\n",
        "\n",
        "    # Loop over all predictions, labels, and images to organize them into the buckets\n",
        "    for index, (score, label) in enumerate(zip(sorted_predictions, sorted_labels)):\n",
        "        for bucket in buckets:\n",
        "            if bucket[0] <= score < bucket[1]:\n",
        "                bucketed_images[bucket][label].append(index)  # Store the index instead of the image\n",
        "                break\n",
        "    return bucketed_images\n",
        "\n",
        "\n",
        "def plot_bucketed_images(bucketed_images, sorted_predictions, sorted_image_id, thumbnails):\n",
        "    num_columns = 2  # (2 for Label 0 and Label 1)\n",
        "    num_rows = len(bucketed_images)\n",
        "\n",
        "    desired_height = 300  # height in pixels for each image\n",
        "    desired_width = 300   # width in pixels for each image\n",
        "\n",
        "    # Create the figure with specified size, considering the new image dimensions\n",
        "    fig = plt.figure(figsize=(20, (num_rows + 1) * (desired_height / 100)), constrained_layout=True)\n",
        "    gs = gridspec.GridSpec(nrows=num_rows + 1, ncols=num_columns + 1, figure=fig,\n",
        "                           width_ratios=[1, 5, 5], height_ratios=[1] + [5] * num_rows)\n",
        "    fig.patch.set_facecolor('#ffe9d2')\n",
        "\n",
        "    # Initialize the axes array using the GridSpec\n",
        "    axes = np.empty((num_rows + 1, num_columns + 1), dtype=object)\n",
        "    for i in range(num_rows + 1):\n",
        "        for j in range(num_columns + 1):\n",
        "            axes[i, j] = plt.subplot(gs[i, j])\n",
        "            for spine in axes[i, j].spines.values():\n",
        "                spine.set_visible(True)\n",
        "                spine.set_linewidth(2)\n",
        "                spine.set_edgecolor('black')\n",
        "\n",
        "    # Setting title rows and column labels\n",
        "    axes[0, 1].set_title(f\"{DIAGNOSIS} Ground Truth - Negative\")\n",
        "    axes[0, 1].axis('off')\n",
        "    axes[0, 2].set_title(f\"{DIAGNOSIS} Ground Truth - Positive\")\n",
        "    axes[0, 2].axis('off')\n",
        "    axes[0, 0].set_title(\"Model output score\")\n",
        "    axes[0, 0].axis('off')\n",
        "\n",
        "    for i, bucket in enumerate(bucketed_images.keys()):\n",
        "        axes[i + 1, 0].text(0.5, 0.5, f\"{bucket[0]:.2f} - {bucket[1]:.2f}\",\n",
        "                            rotation=0, size='large', ha='center', va='center')\n",
        "        axes[i + 1, 0].axis('off')\n",
        "\n",
        "    # Plot images in their respective cell, limited to 3 images max\n",
        "    for i, (bucket, images_dict) in enumerate(bucketed_images.items()):\n",
        "        for j, label in enumerate(sorted(images_dict.keys())):\n",
        "            ax = axes[i + 1, j + 1]\n",
        "            ax.axis('off')\n",
        "            img_count = len(images_dict[label])\n",
        "            indices = images_dict[label][:3]  # Limit to the first 3 indices\n",
        "            combined_image = Image.new('RGB', (desired_width * 3, desired_height), 'grey')\n",
        "            x_offset = 0\n",
        "            for idx in indices:\n",
        "                img = Image.open(io.BytesIO(thumbnails[sorted_image_id[idx]].tobytes()))\n",
        "                img_resized = img.resize((desired_width, desired_height), Image.Resampling.LANCZOS)\n",
        "                combined_image.paste(img_resized, (x_offset, 0))\n",
        "                draw = ImageDraw.Draw(combined_image)\n",
        "                text = f\"ID: {sorted_image_id[idx]}\\nScore: {sorted_predictions[idx]:.2f}\"\n",
        "                draw.text((x_offset + 10, 10), text, fill=\"yellow\")\n",
        "                x_offset += desired_width\n",
        "            ax.imshow(np.asarray(combined_image), aspect='auto')\n",
        "            ax.set_title(f\"{len(indices)} of {img_count} images\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"Display images from the validation set, categorized by the model's prediction score compared to the actual ground truth label. Include up to three images for each category.\")\n",
        "\n",
        "bucketed_images = bucket_images(sorted_predictions, sorted_labels, num_buckets=NUM_BUCKETS)\n",
        "plot_bucketed_images(bucketed_images, sorted_predictions, sorted_image_id, thumbnails)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "EX6YGyHrFbDn"
      },
      "cell_type": "markdown",
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/cxr-foundation/blob/master/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3ac608b8f9188be2227ae82298dfd5de684cbdc4496f362d4b3b9040509447c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}